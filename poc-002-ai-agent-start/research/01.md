# Ubuntu clean install

 Here's a clean and tested **Ubuntu 22.04 install script** to:

1. Install the latest NVIDIA drivers.
2. Install CUDA Toolkit (optional but helpful for some tools).
3. Verify GPU detection.
4. Run a simple CUDA test with Python + `numba`.

---

## ‚úÖ Script: `install-nvidia.sh`

```bash
#!/bin/bash

# install-nvidia.sh
# Tested on Ubuntu 22.04 LTS

set -e

echo "[+] Updating system..."
sudo apt update && sudo apt upgrade -y

echo "[+] Adding NVIDIA PPA..."
sudo add-apt-repository ppa:graphics-drivers/ppa -y
sudo apt update

echo "[+] Installing NVIDIA driver..."
sudo ubuntu-drivers autoinstall

echo "[+] Reboot is required after driver install."
echo "    Press ENTER to reboot now, or Ctrl+C to reboot later."
read

sudo reboot
```

---

## ‚úÖ After Reboot: Check GPU + Run Python Test

### 1. Check if NVIDIA driver is working:

```bash
nvidia-smi
```

You should see the RTX 4070 listed, driver version, and memory usage.

---

### 2. Install CUDA dev packages + Python testing tools:

```bash
sudo apt install -y python3-pip build-essential
pip3 install numba
```

---

### 3. Create Python test script: `cuda_test.py`

```python
from numba import cuda
import numpy as np

@cuda.jit
def add_arrays(a, b, c):
    i = cuda.grid(1)
    if i < a.size:
        c[i] = a[i] + b[i]

N = 1000000
a = np.ones(N, dtype=np.float32)
b = np.ones(N, dtype=np.float32)
c = np.zeros(N, dtype=np.float32)

d_a = cuda.to_device(a)
d_b = cuda.to_device(b)
d_c = cuda.to_device(c)

threads_per_block = 256
blocks_per_grid = (a.size + (threads_per_block - 1)) // threads_per_block

add_arrays[blocks_per_grid, threads_per_block](d_a, d_b, d_c)
d_c.copy_to_host(c)

if np.allclose(c, a + b):
    print("[‚úì] CUDA test passed! GPU computation is working.")
else:
    print("[x] CUDA test failed.")
```

Run it with:

```bash
python3 cuda_test.py
```

---

## üß† What It Tests:
- Loads your GPU via Python + CUDA.
- Checks kernel execution using a simple 1M element vector addition.
- Confirms GPU is usable in your Python AI/ML stack.

---

### üí° Bonus:
Want me to write a follow-up script to auto-install:
- Ollama + LLaMA 3.1
- Python scraping tools
- Vector DB setup (Qdrant)
- Agent orchestration starter?

Just say the word.



# OLLAMA SETUP 
---

## ‚úÖ PART 1: Install Ollama + LLaMA 3.1

Here‚Äôs a script you can use after the NVIDIA setup:

### üìú `install-ollama.sh`

```bash
#!/bin/bash

# Ollama + LLaMA 3.1 installer
set -e

echo "[+] Installing dependencies..."
sudo apt update
sudo apt install -y curl wget gnupg lsb-release

echo "[+] Downloading and installing Ollama..."
curl -fsSL https://ollama.com/install.sh | sh

echo "[+] Starting Ollama service..."
ollama serve &

sleep 5

echo "[+] Pulling Meta's LLaMA 3.1 70B (quantized)..."
ollama pull llama3:70b

echo "[‚úì] Ollama and LLaMA 3.1 are ready to use."
```

> üìç This installs the CLI, the server, and pulls a quantized version (you can also use `llama3:8b` or `13b` for faster use).

---


# Access Ollama from python

## ‚úÖ PART 2: Accessing Ollama from Python

Ollama exposes a **REST API on `http://localhost:11434`**.  
So in Python, just use `requests` or any HTTP client.

### üîß Install the Python client dependencies:

```bash
pip install requests
```

---

### üìú `ollama_agent.py` ‚Äî Minimal Ollama Python Client

```python
import requests

def query_ollama(prompt, model="llama3:70b"):
    response = requests.post(
        "http://localhost:11434/api/generate",
        json={"model": model, "prompt": prompt, "stream": False}
    )
    response.raise_for_status()
    return response.json()["response"]

if __name__ == "__main__":
    prompt = "Summarize this text: The Philippines is seeing a boom in AI startups..."
    answer = query_ollama(prompt)
    print("[Agent] Response:", answer)
```

---

## ‚úÖ PART 3: Preferred Way to Use LLaMA in AI Agents

Now you‚Äôve got 2 key options in Python:

---

### Option 1: **Directly using Ollama in LangChain (Recommended for Agents)**

LangChain already supports Ollama:

```bash
pip install langchain langchain-community
```

```python
from langchain_community.llms import Ollama
from langchain.agents import initialize_agent, Tool
from langchain.agents.agent_types import AgentType

llm = Ollama(model="llama3:70b")

# Define a simple tool
def my_tool(input_text):
    return f"Got input: {input_text}"

tools = [
    Tool(name="CustomTool", func=my_tool, description="Echoes input")
]

agent = initialize_agent(
    tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True
)

agent.run("Use CustomTool to handle: Hello world!")
```

> üí° **LangChain** gives you memory, tools, and agent orchestration right out of the box.

---

### Option 2: **Use CrewAI (modern agent framework)**

```bash
pip install crewai
```

Sample starter with Ollama as the backend is possible, but it‚Äôs still experimental. You can plug it via a custom LLM wrapper using `requests`.

---

## üß† What I Recommend

| Layer | Tool |
|-------|------|
| Inference | **Ollama** |
| Python access | `requests` or `LangChain` |
| Agents & tools | **LangChain Agents** (CrewAI optional) |
| Orchestration | Local async jobs or `FastAPI` task manager |
| Memory | `Chroma` / `FAISS` or `Qdrant` |
| Storage | SQLite / PostgreSQL for storing outputs |

---

## ‚úÖ TL;DR
- Use **Ollama** to serve LLaMA 3.1 locally.
- Access it via **Python `requests`** or **LangChain‚Äôs Ollama integration**.
- Build agents using **LangChain Agents** or **CrewAI**.
- You now have full control of your LLMs + AI agents in your homelab.

---

Want me to help you set up a **template AI Agent repo** for this? I can scaffold a full system with:

- `agent_runner.py`
- `scraper.py`
- `summarizer.py`
- `strategist.py`
- `.env` + DB schema for long-term storage.

Your AI Agent repo scaffold is now complete and includes:

### üìÅ Files included:
- `agent_runner.py`: Main orchestrator for scraping, summarizing, and strategizing.
- `scraper.py`: Uses `yt-dlp` to extract YouTube transcript text.
- `summarizer.py`: Sends text to Ollama LLaMA 3.1 for summarization.
- `strategist.py`: Sends summary to Ollama for strategic analysis.
- `.env`: For environment variables like model name.
- `db_schema.sql`: SQL schema to store summary + strategy output.

---

### ‚úÖ Next Steps:
1. Install requirements:
   ```bash
   pip install yt-dlp requests python-dotenv
   ```

2. Run with:
   ```bash
   python agent_runner.py
   ```

3. [Optional] Connect to a SQLite/Postgres DB to store results using the schema provided.

---

Want me to add database save logic next or dockerize this whole thing for deployment in your homelab?