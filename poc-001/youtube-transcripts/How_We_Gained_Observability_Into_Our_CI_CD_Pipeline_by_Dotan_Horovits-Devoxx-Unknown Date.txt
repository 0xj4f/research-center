hey everyone uh thanks for uh joining me on this talk glad to be here uh at Vox days Brussels and today I'd like to talk to you about uh how we gained observability into our cicd pipelines and uh how you can too I'll use Jenkins as a reference but the focus is on principles and best practices so even if you're not Jenkins users uh definitely applicable to uh many other tools of your choice and I'd like to start with a day in a live of uh one of our Engineers or our developers on duty the on call engineers at least what used to be before the uh change that I'm going to uh talk to you about so as a developer on duty or an on call engineer that's how the day used to look like first thing in the morning you start the day um with your coffee checking going over the Jenkins pipelines and um check what happened uh what went on in the last few hours uh most importantly of course if there are any Reds uh probably you know these the signal very well and uh especially if there's a red Master on the master Branch uh and essentially if you can finish your uh coffee uh or if you need to jump in and see uh what the heck is going on so that's what uh used to be and if there is obviously uh one red of these you need to uh essentially go into the runs the different Jenkins runs one by one and start understanding uh what happened where it failed why it failed uh and so on and when I mean going one by one I mean uh there different pipeline runs so you have the front end you have the uh backend you have smoke tests you have many others so going into each and every one to understand the pattern across the different runs across different uh branches and so on and maybe another very important point is that there was no uh uh easy way to compare that also with historical uh pipeline runs historical data so that's uh that's our journey and uh the types of questions that uh we wanted to ask and maybe it wasn't that uh easy and straightforward to answer a few examples you can see here uh did all runs fail uh on the same step did all runs fail for the same reasons and how many failed on that reason uh did the failure occur in a specific Branch maybe on a specific machine uh which failed the most if you see suddenly a spike in the latency U taking longer is that good or not what's what's the Benchmark even for the normal steady state so these are the sorts of questions um that it took us too much time time to answer and we realized that we need to improve that's that was the beginning of our journey with uh the cicd my name is Doan horvitz uh that's a tweet from uh my last visit to uh to Brussels back in February for those who know fost them config management Camp the open source month uh and I also love Belgian beers so uh always happy to come back here um and I'm the principal developer Advocate at logs. logs iio provides a cloud native observability platform that's based on popular open source tools such as uh uh open search Prometheus Jaga open Telemetry and others and makes it available as a unified platform built for scale uh I'm also a cncf Ambassador cncf is the cloud native Computing Foundation the open source Foundation under which uh you can find the kubernetes and Prometheus and Argo and probably many other projects that you know and love uh so if you have any questions about the uh cncf ecosystem or anything related feel free to uh catch catch me up after after the talk happy to talk about any of the cncf ecosystem Cloud native ecosystem and I also have a podcast called uh open observability talks in English about open source devops observability so if you're interested in any of these topics uh do check it out on any of your favorite podcast apps or on YouTube um and as you can see you can find me everywhere at horror vits so uh if you want to reach out afterwards and you're shy to ask your question feel free to reach out or if you're tweeting anything interesting out of this talk feel free to uh to tag me so going back to our journey before h i I go into how we how we improved our our cicd I first want to understand what we need to improve and I see many users actually jumping into improving without re really clearing uh and defining the kpis of what they want to improve on so this is very basic thing um and for that I I really love the the door metric system who who knows the door metrics with a show of hands okay we have only one person so good thing I've prepared this slide I'll go over that maybe a bit slower than uh to to show that dor metrics essentially dor metrics for devops it started as a resent uh to understand what makes high performing teams then back in 2018 uh and by the way you have a a QR code here that you can scan with your phone if you want a good uh uh overview on like 101 on dorom metrics but essentially back in 2018 the team published a a very famous book The accelerate book uh in which they identified essentially four different uh key metrics that indicate the uh performance of software development teams and this is the the point that I want to uh share with you four uh different metrics first is the uh deployment frequency uh which is essentially how often an organization successfully releases to production then there's the lead time for changes or cycle time sometimes called which is the amount of time it takes a commit to get all the way to uh production then there's the change failure rate the percentage of deployment causing failure in production and the fourth metric is the time to restore service or meantime to recovery mttr which is uh essentially how long it takes an organization to recover from a failure in production so these are the four uh dor metrics very very useful to track and uh quantify the uh how effective essentially your software delivery life cycle is in in various terms the the first two you can see are more on the developer side the the uh last two are more on the op side so it covers really the the uh the full spectrum and as I mentioned you need to understand what what matters to you and what you're trying to optimize on uh in our case uh we we needed to improve on the lead time uh for changes that the amount of time it takes a commit to get into production which because of the failure of the the pipelines took us longer than we uh uh it was too high and was holding us back essentially so now that we know what we want to um improve on the question is how to get into that and uh as I mentioned at logo we're experts in observability that's that's what we do for a living so uh we very quickly understood uh that uh we're missing essentially observability into our cicd pipelines uh and as I mentioned before we use Jenkins as our our tool and Jenkins does provide did provide us with some observability out of the box so you could get into the uh specific pipeline run and then see the different stages of that run and how much time was spent on each one of these stages so you got some observability we even hooked it up with Jenkins with our slack so we got some alerts on failures so we did some what we could with Jenkins of the box but and that's very important but it wasn't good enough for us and I want to share with you not just again even if you don't use Jenkins just for you to understand the parameters and the aspects that you need to assess in your tool so in our case we wanted to find a way to Monitor aggregated and filtered information uh across all the pipeline runs across all branches across all machines uh in order to see the full picture on a scaled time range uh with our uh own filtering demands and our own needs just like you do in production essentially the way that you monitor production we wanted to be able to monitor our cicd pipelines so we went into this journey of improving our cicd observability with these four goals for the project first one is uh was uh achieving dashboards with aggregated views uh to see across pipelines runs branches as I mentioned before the second goal was to get historical data to be able to understand the trends and to identify patterns AC over time thirdly we wanted uh reports and alerts so that we can automate as much as possible and lastly and very importantly we also wanted to have observ it into our test performance to be able to see uh flaky tests test performance to understand their impact on our pipeline so these were the goals remember them we'll get back to them later and now that you understand what we want to achieve in this project uh let's talk about how to achieve that and essentially takes four steps to achieve collect store visualize and report in terms of the tech stack we have a lot of expertise uh with the elk stack elastic search uh Cabana who who uses who knows elastic search Cabana the elk stack okay quite a few um uh so we started with that um on our journey and I want to show you how these four steps are achieved with uh this specific texto and let's start with the first step collect so for that end we instrumented the pipeline this is an example of a Jenkins pipeline run and we instrumented the pipeline to collect all the relevant information uh and put it in Environ variables okay what kind of information am I talking about so here are a few examples you can see uh the branch the kicha the uh machine IP the Run type uh namely schedule triggered by merge to master or by uh uh push to Branch um failed Steps step duration obviously build number a pipeline status whether it's a past whether it's flaky so all all the essentially all the information useful information you can get put it store it in environment variables and then let's move on to the second step the store step after collect uh and for that end we created the new summary step that you can see here in red uh the summary Step at the end of the pipeline where you ran a command to collect all the pipeline information from the environment variables and essentially uh create a Json uh with the pipeline and then store it uh in on elastic search uh we have our own so for us it was a is a no brainer we have a managed Service uh and by the way now move to uh open search I don't know if you know elastic search is no longer open source unfortunately since a few uh uh years ago a couple of years ago so open search is the elastic search open open source Fork of elastic search and Cabana so we moved since we're based on open source stack uh we move to open search and now we use open search and open search dashboards for the visualization and how many uh use open search just with a show of hands none okay um so one point we talk about the store phase and the question that often times comes up in my talks is uh what about Jenkins builtin persistence and and there are and buil-in persistence is available in jenin but and we tried it but there are few downsides uh so first of all by default Jenkins keeps all the uh builds and stores them on the Jenkins machine itself uh which burdens these machines these are the critical path of our of our pipeline right uh and then you need to start limit how many builds to keep and for how long how many days and so on so uh and this was very limited uh and we wanted to persist historical data in our control uh the duration the retention and very importantly off of the jenin server so as not to load uh the critical path and we also wanted more powerful access to the historical data to analyze the same way we analyze essentially our production environment so uh this is this is why we went into this uh this journey so we talked about collect store and now let's talk about visualize so now that all the pipeline run data is in elastic search it's very easy to build Cabana dashboards uh and visualizations or if you store it in open search then with open search uh dashboards and the question is which visualizations shall we build and here I want to take you back to uh the beginning of the talk we talked about the questions that we wanted to answer because this is the guideline that that should lead you uh you can use these questions but the purpose is not to memorize my questions but rather to formalize your own questions and to let them Define the visualizations and the observability that you need so we talked about before questions such as did all runs fail um on the same St step did all runs fail for the same reason uh um did the failure occur in a specific uh Branch or in a specific machine uh and uh and U if something takes longer is that normal what's the Benchmark so these are the questions that helped us Define the visualizations and I want to give a few examples again it's example so uh use them as a reference to guide your process in assessing and defining your observability but let's look look at some some examples here and the first one is the Top Line uh status check uh how stable the pipeline is remember that uh on call engineer waking up in the morning with a cup of coffee opening the uh the Jenkins wanting to understand if everything's okay and they can finish the coffee this is a very good visualization to start the day you can see here the success versus failure rates uh in general or a specific uh time range so you can also choose the type of visualization that that fits you and as you can see it's very easy to see suddenly a spiking something red that may call for your attention so that's the Top Line another example finding problematic steps remember the question so uh visualize failures segmented by pipeline steps again very easy to suddenly see a spiking uh uh step whether uh visually or on the listing uh very nice visualization another example detecting problematic build machines uh so for that we visualized failures segmented by Machine uh and that by the way was very very useful because uh it saves wasting time looking for a bug in the release code when you see something like that you can just kill that instance let the autoscaler spin up a new instance and many times this solves the problem so uh it's very very useful uh to understand whether it's environmental or code based issues and a lot of our uh uh runs were actually environmental so much so that I will get back to environmental later and show you how you can even dive deeper into the environmental uh issues in your pipelines next duration per step if you suddenly have latency what's the normal what's the Benchmark is that really an anomaly so for that uh if you remember before collecting the cicd pipeline data we we could enter into specific runs and see the specific step how much it took in that specific run but what we what we lacked is uh uh to monitor aggregated information across runs uh across builds across machines and see the trend so this is the way to uh now see that as a graph and understand if this is really an anomalous Behavior or not so just to summarize uh these are as I said just example visualizations you'll create your own based on your questions and your uh needs for observability so after we talked about collect store and visualize the last step is report and as you as you remember before the developer on duty uh needed to remember to manually enter uh Jenkins to check the pipelines now life is much easier much better you get uh daily start of day report on slack as you can see here on the screenshot you get notifications and the notification is actually linked to the dashboard so with a click of a button you can jump straight into the dashboard and see what what goes on what the in the past hours uh and there's also an embedded snapshot as you can see so you don't even need to get into the dashboard you see and if everything is green you can finish your coffee but if something uh catches your eye then you go in and uh investigate further um and you can also Define alerts on the data just like uh just like anything you can Define triggered alerts with complex conditions on any of the um data that we collected on the collect phase so can do if some of failures goes above X or uh um the average duration goes above y uh set an alert so essentially anything that you can Define as a Lucine query I see that many know elastic search so anything you can Define as a Lucine query you can create an alert and uh and uh automate it um and this is done based on an alerting mechanism we built on top of elastic search and open search so it's very easy to uh to achieve um and also it's it's not just for slack we work with slack in our engineering and devops but uh you can send it to many other uh notification endpoints such as pager Duty Ops gen Ms teams and more so you can even add your own custom endpoint using web hooks so we personally worked with the slack But whichever Downstream system you work with so that was the uh um observability uh that we were looking for but that was Jenkins but cicd is more than Jenkins so what else what else do we want to uh achieve let's see who listened in the beginning of the uh the project statements what they haven't we covered yet sorry what do you mean deployments no we're talking about the uh the the pipeline run so it's cicd observability the production actually was well covered before that project so the fourth element in the four statements uh goals was the um test performance I don't know if you remember that we talked about test performance so uh we wanted to analyze flaky tests and test performance and the process is essentially the same uh following the same process we collected the relevant information from our test runs and stored them in elastic search then created Cabana dashboard that you can see here and all the visualization that you probably would like to have uh uh let's see test duration fail tests flaky tests uh failure count and failure rate moving averages fail tests by Branch over time flaky test Suites over time so really all the things that you would be looking for visualized uh with cabana and following the same process of course after uh visualize there's the report phase and we created for that as well a dedicated build alerts uh app build alerts as you can see slack Channel and in the slack Channel whoever was in Duty got a a slack message notification with a report summary link to the dashboard and so on so really the same process that I talked about collect store visualize and Report one very important point to mention is the flexibility and openness different teams uh created different visualizations over the data uh to suit their needs and their style so once you have the data in elastic search it's very flexible and each team can have I took the Other Extreme example a different team that didn't like any graphs whatsoever they like the lists and they did all of the visualization of the test Suites based on lists uh I I talk about um quite a bit about platform Engineering in different uh uh different uh events and different forums and this is actually a part where who has a platform Engineering in in his organization with the show of hands no one okay so but if you do have someone that provides sort of a platform infrastructure this is a great practice the platform engineering can prepare the data and then make it accessible for the teams and then the teams have the flexibility each one to consume and to visualize uh and create their own reports and alerts based on their preferences flexibility and and openness so just to summarize how we gained observability first first we instrumented Jenkins pipeline uh to collect all the relevant data and put it in environment variables then at the end of the pipeline we created Json we all the data and stored it in elastic search or open search and then we created Cabana visualizations on top of that data and lastly we created reports and alerts on the data okay four steps collect store visualize and report so we have that what's next next we wanted to uh investigate the performance of specific pipeline runs okay and what do you need to do in order to do that we showed some visualization before in Cabana visualization where you saw the visualized per step duration right that was a good start but it wasn't enough to really investigate the sequence to see really the sequence over time uh and that's what distributed tracing is for uh how many uh know and use distributor tracing not for CD in general like production okay so we have we have a few in the audience I'll say a word about that uh but essentially that's really uh something that suits very nicely and Jenkins the nice thing is that Jenkins can emit traces Trace data uh just like it emits logs so it was very easy to integrate uh so we decided to visualize jobs and pipeline executions as distributed traces so uh a word about distributor traces for for those of the audience that don't know uh distributed tracing essentially helps to pinpoint where failures occur and what causes poor performance in your uh in your system in your production system so the primary use is actually not for cicd pipelines but for uh for production environments uh and this is best suited for distributed applications for microservice architectures where each request coming into the system goes through a chain of interacting microservices uh Downstream and then when suddenly you get I don't know a 404 at the end point an error and you want to understand where the error comes from within this chain of of calls or if you get very high latency suddenly it takes I don't know 200 milliseconds to complete login or or some other API call you want to understand where this latency is coming from in the downstream systems these are the sorts of questions that distributed tracing helps you answer and the way it works is that each uh call in the chain uh you in our case each step in the pipeline uh creates and emits a span which you can think about a span like a structured log that contains the the name of the operation that was invoked the method or what not uh the name of the step the duration of that step uh and an additional uh additional data essentially meta data and then there's a back end that collects all these uh spans reconstructs the full Trace based on causality and then visualizes uh typic using the the gun chart visualization that you see on the on the right hand side so it's very easy to see uh a called B then B called C and then D and then when B finished then a called e you really see the sequence you see how much time each call took which calls ran in parallel which ran sequentially and so on this is the power of distributed tracing so let's go back to our process following the same uh the same sequence that we talked about before we start with collect step and for the collect step we decided to use open Telemetry collector how many here you know open Telemetry heard of the project open Telemetry open source okay so we have a few uh so a word about open Telemetry open Telemetry is a is an open source framework for uh generating and collecting Telemetry data from your system it could be logs metrics traces actually very recently added the also continuous profiles uh so very diverse so essentially one framework to to rule them all all of the Telemetry types uh it's an open source under the cncf the cloud native Computing Foundation um and um uh you have a QR code here for a guide on open Telemetry I hope that you can see that so if you want to learn more about Open Telemetry uh check out The Beginner's Guide um and at the time open Telemetry was already uh GA for distributor tracing so we decided to go with that for the instrumentation part so the process was as follows we uh um essentially set up an open Telemetry collector as a separate component an agent if you'd like and then installed on the Jenkins the Jenkins open Telemetry plugin you have the URL here uh and then all you need to do is to configure the Jenkins uh open Telemetry plugin to send the open Telemetry uh collector to the end endpoint via OTL over grpc protocol that's all it takes for the collect phase and after collect comes store uh for the back end we used Jager uh who knows Jager project okay so we have a few um Jagger is is a very popular open source Tool uh for distributed tracing it's also a project under the cncf the cloud native Computing foundation so and we use J Jaga uh to monitor our production environment so we uh also and we have it as as our own managed Jagger service so for us it was a no-brainer to use that also for the cicd pipeline uh but I'm showing the case here for uh Jenkins even if you selfhosted you have even a a link here if you want to a guide on how to deploy Jager and kubernetes in production so check out that URL works just the same so uh as we use the AER for the back end we need to configure the open telemetric collector essentially so open telemetric collector the trace data is aggregated and then sent to the Jaga uh back end for storage and indexing and then after store comes visualize here it's it's much easier than with cabana you don't need to start building your own visualizations they're built in visualizations uh simple case Yi has this uh gun chart that I talked about before four and as you can see it's very easy to see the step sequence of the of the Run uh on the right you see uh on the right hand side you see each uh let's start with the left actually you can see this tree structure that shows you the nested call so you really see uh which one call calls which and then on the right hand side you can see the length the duration of each step um and the sequence how much time each uh each step took and which one is the one that is running in parallel which one runs sequentially so for example these two ran in parallel but these ones only when this finished this one started so it's very easy to see where the latency is coming from where the time is spent uh and so on uh and yger offers besides the timeline view also other views like a dependency graph view Trace statistics flame graph was recently added on the on the button here on the on the top uh where you see Trace timeline so this is but this is the most uh most used ones this is an example of the graph view uh and as I mentioned on when you click on the button on the right top right you see the other options uh for visualizations really uh really powerful visualizations built in this is the graph flame graph that was just recently added uh so really cool visualizations so that was adding uh traces to our Jenkins runs but as before cicd is more than Jenkins so what's next what you can late to do is to add instrumentation to uh Maven in this example or it could be anible artifactory uh and other tools to get finer granularity uh of the traces and the steps uh in this example as I said the yellow what you see in yellow is Maven so what before appeared like one long black box span now you can open it up and see the different uh build steps that uh the breakdown into the different build steps within Maven so it gives you finer granularity uh to understand where things uh take time um uh and similarly as I mentioned uh um you can find other common Frameworks having integration with open Telemetry like anible plugin uh for testing you have Java junit uh Jupiter plugin python and so on so really very powerful um and uh this is where you can take it next step after the Jenkins itself so we've seen how to uh what it takes to add tracing to our cicd pipeline following the same steps collect store visualize and of course you can create alerts on the traces as well just like we've done before with the with the log DOA so what's next as I mentioned before when we talked about uh uh the the causes for uh pipelines failing many of the pipelines fail not because of the U released code but because of uh the cicd environment itself environmental issues so before I showed on the Cabana dashboard that we see uh suddenly a spiking machine that uh with higher rate that's fine but we wanted finer granularity we wanted to uh monitor metrics from the Jenkins server and the environment the system the containers even the jvm uh anything that essentially can fail irrespective of the release code so that was the same uh the next step in our journey and following the same uh steps collect store visualize and report so let's see how how it's done for the collect phase uh we used Telegraph um actually today we're going to open Telemetry that or already supports metrix but back in the day metrix was not GA in open Telemetry so we went with Telegraph uh uh for and we use that in production so we use that as well uh in our uh in our system Telegraph uh who knows Telegraph with showand no one so Telegraph is is part of the um uh open source Suite by influx data so it comes together with influx DB and and others there's a tick stack for those who know the their stack uh but just so you know you can do the same with open Telemetry or other shipers just an example essentially collect phase has two steps as you can see here first we need to configure Jenkins to expose metrics uh in a Prometheus format we use Prometheus formats extensively in our production so that was our natural uh choice for monitoring uh and that's a simple configuration in the Jenkins web UI uh and then the next step is to install a telegraph instance if you don't already have one running and configure Telegraph to scrape the metrics uh uh off of the Jenkins servers with a Prometheus uh input plugin that you can see here um Prometheus input plugin and that's all it takes for the collect phase and then after collect we have the store I mentioned that we use Prometheus extensively uh who uses Prometheus just out of curiosity for metrics okay so we have a few uh so Prometheus again another open source uh database time series database to collect metrics time series data uh we use it in production uh so we follow the same for the cicd pipeline we have also our own managed Prometheus so it's a no-brainer just plug it in but it works with a self-manage Prometheus the same way um um you just essentially configure Telegraph to send the metrics to Prometheus and you have two options for that you can do it in a pull mode or in a push mode so if you do it in a pool mode uh you use the standard uh pool mode is the standard Prometheus way so if you have your own Prometheus instance it will by default scrape in a pool mode that's called scraping in prometheous terms um and then all you need to do is to configure Telegraph to expose a a sort of a slash matric endpoint and the telegraph will scrape that endpoint so and for that you just configure the promises client output plugin if you want it in a push mode so that the telegraph pushes to your back end of choice then use the HTTP output plugin and remember to set the data format to promus remote ride uh just as a format uh to be used so that's the uh store phase and after store once we have everything in Prometheus obviously very easy to build grafana uh dashboards on top of it and here you again you'll build your own visualizations just like we did with cabana based on what you want to do but just to give you ideas and and first of all the let's talk about the filters you see the filters here at the top so being able to filter by build type by Branch by machine ID by build number remember all these things that we talked about before that you want to be able to very clearly understand if it's a branch issue if it's a machine issue so the filters give you all this uh capabilities and of course you can monitor the system metrics what you can see here on the uh on the screenshot like CPU memory dis uh usage load Trend over time all the things on the system level uh then you can have uh also monitoring the docker container metrics uh as you can see here some examples the container CPU memory IO network uh dis us usage behavior and of also of course the running stopped posed containers by Jenkins machine uh so and as we said we can filter all of that to investigate a specific machine a specific build specific build type uh a specific Branch or any combination of the uh filters above so that's on the container visualization we also uh show the jva metrics of the Jenkins machine uh such as the as you can see here the when you can see thread count heat memory usage garbage collection duration all the things that you would expect on the jvm level that can screw up our um mess up our our pipeline run and of course you can monitor the Jenkins nodes and cues and execute uors themselves these are the native Jenkins entities so you can see here monitoring the the Q size and Status breakdown buildable blocked pending stuck all these statuses you can this the cues and then the jobs the count of executed over time the break breakdown by job status how many aborted how many failure how many not built success unstable and so on the job duration uh so uh these are uh so useful that actually we made some of these available as a as a uh pre-built dashboards that even our users can install with a click of a button because it's it was really very useful to uh not just to us apparently to others as well so uh pretty cool uh and we can also set alerts on these metrics just like we said so after H visualize we can do reports um so I want to summarize uh what we've seen here in this journey I know it's a lot of information but the principles are what matters first treat your cicd just like you treat your production environment just like you use elastic search or open search Prometheus grafana Jager to get observability into your production environment do the same with your cicd uh pipelines and also leverage the same stack so if you have a stack that works for you and your team for production don't reinvent the wheel just use the same try and reuse as much as possible for your cscd pipelines as I said we embarked on this project with these goals in mind first we wanted dashboards to see uh aggregated and filtered information from several pipelines uh uh belonging to different runs different branches in our time range of choice we wanted historical data and controlled persistence off of the Jenkins uh servers as as we mentioned uh and to determine the duration the retention of the data in our control we wanted reports and alerts to automate as much as possible and lastly we wanted uh to see flaky tests and test performance in general and their impact on on the pipeline run so that's the journey and this is how we achiev that in a summary and again four steps if there's one thing to take out that you take out of this stock take this uh principle four steps to achieve uh observability collect store visualize and Report uh collect is instrumenting your pipeline to get the events the metrics the traces then store and visualize according to the data type um elastic search open search for uh um logs and events Prometheus and grafana for metrics Jager for traces this is the stack that I showed here of course you can use your own stack and then get H uh alerts and reports to automate um so this is the the principle it could be self-manage it could be log Zio as a unified uh platform that's my employer so uh a hook to my employer if you want to check out the the free trial for that and very important what we gained out of this journey okay let's uh zoom out the first thing we got significant Improvement uh in our lead time for changes if you remember the DOR metrics we saw at the beginning significant uh Improvement there also improved uh on call experience the developer on duties uh experiences much much uh more friendly these days uh and also it's based on the open source stack that we know and love so nothing new to learn in order to achieve that um this is our journey if you want to uh learn more about uh cicd observability I wrote a guide uh to see ICD observability you can scan this QR code it gives a lot more information about each and every one of these steps that I talked about uh here and the different open source tools that you can use for each one uh and so on and one final note that I want to say is about uh standardization on cicd observability so we've sort of reinvent we reinvented the wheel we as I said we collected different types of data we H saved it in environment variables or some some other mean but um the question is is there sort of a standard uh way to do that and until very recently there hasn't been and I'm glad to say that uh We've started a a working group under the cncf the cloud native Computing Foundation to actually standardize on cicd observability as part of the open Telemetry open source project or specification uh so there's now an active uh uh special interest group on formalizing the semantic conventions needed to uh make that the vision is essentially that once we have unified semantic conventions on these cicd events then each tool can then emit uh the its cicd data in a uniform Manner and then you don't need to re vend the wheel you can use whichever tool you choose but you can uh it will be uniform across the different uh uh instrumentation tools of choice so whether it's Jenkins or Circle Ci or GitHub actions or whatnot you'll have each one of them will emit in the same way and then you choose the the way that you collect and and visualize and Aggregate and so on Downstream that's a very cool uh working group that I'm glad I was one of the initiators and if you are interested in this domain and if it's something that you're passionate about it's open source it's Community Driven so uh do reach out to me afterwards happy to give you more information and see and you can actually take part and help us shape what these semantic conventions look like what the standardization should look like uh for for greater good as as a community so uh this is about that and uh again you have the guide if you want some more information on that uh with that I'll uh wrap up uh it's still the month of May so I'll also uh uh greet you uh what I said a couple weeks ago when I was at devox UK May the open source be with you and thank you very much for listening [Applause]